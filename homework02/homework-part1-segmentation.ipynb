{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03840993",
   "metadata": {},
   "source": [
    "**Neural networks for road segmentation**\n",
    "========================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7def38b0",
   "metadata": {},
   "source": [
    "Your next task is to train neural network to segment road on images from car cams."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdf93db",
   "metadata": {},
   "source": [
    "The original dataset is provided by Karlsruhe Institute of Technology (http://www.cvlibs.net/datasets/kitti/eval_road.php). Their images are Â±370x1270, but, for simlictiy, we will use 370x370 squares."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2072b7bb",
   "metadata": {},
   "source": [
    "One can download the dataset from https://disk.yandex.ru/d/QPOw4hk84-se_w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fec3016",
   "metadata": {},
   "source": [
    "Here is an example of input data with corresponding ground truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07565b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6665d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paths(path):\n",
    "    _, _, filenames = next(os.walk(path))\n",
    "\n",
    "    images_paths = []\n",
    "    for filename in sorted(filenames):            \n",
    "        images_paths.append(os.path.join(path, filename))\n",
    "    \n",
    "    return np.stack(images_paths)\n",
    "\n",
    "class RoadDataset(Dataset):\n",
    "    \"\"\"Feel free to rewrite it. For ex. cache all images in RAM to increase training speed\"\"\"\n",
    "    def __init__(self, images, masks, transform, aug=None,):\n",
    "        self.images = sorted(images)\n",
    "        self.masks = sorted(masks)\n",
    "        self.transform = transform\n",
    "        self.aug = aug\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):        \n",
    "        im_name = self.images[idx]\n",
    "        mask_name = self.masks[idx]\n",
    "        \n",
    "        image = cv2.imread(im_name)[:,:,::-1] / 255.0  # ::-1 to convert from BGR to RGB\n",
    "        mask = cv2.imread(mask_name, cv2.IMREAD_GRAYSCALE)\n",
    "        mask = (mask > 0).astype(np.float32)\n",
    "        \n",
    "        if self.aug:\n",
    "            sample = self.aug(\n",
    "                image=image,\n",
    "                mask=mask,\n",
    "            )\n",
    "        else:\n",
    "            sample = {\n",
    "                'image': image,\n",
    "                'mask': mask,\n",
    "            }\n",
    "        \n",
    "        sample['image'] = self.transform(sample['image']).float()\n",
    "        sample['mask'] = self.transform(sample['mask']).float()\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f2b951",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = get_paths(\"data/train/images\")\n",
    "y_train = get_paths(\"data/train/gt\")\n",
    "\n",
    "X_test = get_paths(\"data/test/images\")\n",
    "y_test = get_paths(\"data/test/gt\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc255049",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = RoadDataset(X_train, y_train, transform)\n",
    "test_dataset = RoadDataset(X_test, y_test, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9056519f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = train_dataset[4]\n",
    "image = sample['image']\n",
    "mask = sample['mask']\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image.permute(1, 2, 0))\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(mask.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbc3dcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7a8986f",
   "metadata": {},
   "source": [
    "Workflow:\n",
    "---\n",
    "* Choose correct loss function, write training loop and code for testing. Feel free to use previous HW for inspiration. \n",
    "* Train any segmentation neural network from scratch (for example U-Net) and achieve >= 0.75 IoU on test set (40% points). See function to calculate the metric below.\n",
    "* Use any pretrained model for image classification, convert it for segmentation by adding decoder (don't forget skip-connections) or usign dilated convolutions and achieve >= 0.87 IoU  on test set (60% points).\n",
    "\n",
    "You're not allowed to do only one thing: train your network on test set.\n",
    "\n",
    "----\n",
    "Your final solution will consist of an ipython notebook with code (for final networks training + any experiments with data) and test metric calculation.\n",
    "\n",
    "Feel free to ask in Telegram chat if something is not clear :3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91bc236",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dataset length {len(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057ee74d",
   "metadata": {},
   "source": [
    "Dataset is small so actively use data augmentation: rotations, flip, color-change etc. to prevent overfitting.\n",
    "\n",
    "Most likely you'll have to pad your images to 512x512 (it divides by 2^5=32, like U-Net wants). Use PadIfNeeded from Albumentations and central crop (see below) after prediction to calculate loss/metrics (you don't want to pay attention on padded values).\n",
    "\n",
    "----\n",
    "There is a hard data class imbalance in dataset, so the network output will be biased toward \"zero\" class. You can either tune the minimal probability threshold for the \"road\" class, or add class weights in optimized loss. You also can try to use softIoU or DICE loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0dcf8f",
   "metadata": {},
   "source": [
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a8610c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_iou(prediction, ground_truth):\n",
    "    n_images = len(prediction)\n",
    "    intersection, union = 0, 0\n",
    "    for i in range(n_images):\n",
    "        intersection += np.logical_and(prediction[i] > 0, ground_truth[i] > 0).astype(np.float32).sum() \n",
    "        union += np.logical_or(prediction[i] > 0, ground_truth[i] > 0).astype(np.float32).sum()\n",
    "    return float(intersection) / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0483e274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def central_crop(images, size):\n",
    "    current_size = images.size(2)\n",
    "    border_len = (current_size - size) // 2\n",
    "    images = images[:, :, border_len:current_size-border_len, border_len:current_size-border_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e06e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, data_loader, optimizer, criterion):\n",
    "    loss_history = []\n",
    "    iou_history = []\n",
    "    \n",
    "    for x_batch, y_batch in data_loader:\n",
    "        print(1)\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        print(2)\n",
    "        output = model(x_batch)\n",
    "        print(3)\n",
    "        loss = critetion(output, y_batch)\n",
    "        print(4)\n",
    "        optimizer.zero_grad()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_history.append(loss.item())\n",
    "        iou_history.append(calc_iou(output, y_batch))\n",
    "        \n",
    "    return loss_history, iou_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665380da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, criterion):\n",
    "    loss_history = []\n",
    "    iou_history = []\n",
    "    \n",
    "    for x_batch, y_batch in tqdm(data_loader):\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        output = model(x_batch)\n",
    "        loss = critetion(output, y_batch)\n",
    "        \n",
    "        loss_history.append(loss.item())\n",
    "        iou_history.append(calc_iou(output, y_batch))\n",
    "        \n",
    "    return loss_history, iou_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ba6249",
   "metadata": {},
   "source": [
    "### Model U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11baf5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.transforms import CenterCrop\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67347b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_channels, output_channels, \n",
    "                 kernel_size=(3, 3), stride=1, dilation=1, use_dropout=True):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_channels, \n",
    "                               out_channels=output_channels,\n",
    "                               kernel_size=kernel_size,\n",
    "                               stride=stride,\n",
    "                               dilation=dilation)\n",
    "        self.conv2 = nn.Conv2d(in_channels=output_channels, \n",
    "                               out_channels=output_channels,\n",
    "                               kernel_size=kernel_size,\n",
    "                               stride=stride,\n",
    "                               dilation=dilation)\n",
    "        self.dropout2d = nn.Dropout2d(0.2)\n",
    "        \n",
    "        self.use_dropout = use_dropout\n",
    "        \n",
    "    \n",
    "    def forward(self, input):\n",
    "        x = self.conv1(input)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        if self.use_dropout:\n",
    "            x = self.dropout2d(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebaa2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownScaller(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_channels, output_channels,\n",
    "                 kernel_size=(3, 3), stride=1, dilation=1, use_dropout=True):\n",
    "        super().__init__()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size = (2, 2))\n",
    "        self.conv_layer = ConvLayer(input_channels, output_channels, kernel_size,\n",
    "                                    stride, dilation, use_dropout)\n",
    "    \n",
    "    \n",
    "    def forward(self, input):\n",
    "        x = self.maxpool(input)\n",
    "        x = self.conv_layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccf689e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpScaller(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_channels, output_channels,\n",
    "               kernel_size=(2, 2), stride=2, dilation=1, use_dropout=True):\n",
    "        super().__init__()\n",
    "        self.conv_layer = ConvLayer(input_channels, output_channels,\n",
    "                                    kernel_size=(3, 3), stride=1, dilation=1)\n",
    "        self.conv_transposed = nn.ConvTranspose2d(output_channels, output_channels // 2,\n",
    "                                          kernel_size=kernel_size, stride=stride)\n",
    "    \n",
    "    \n",
    "    def forward(self, input):\n",
    "        x = self.conv_layer(input)\n",
    "        x = self.conv_transposed(x)\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90550fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Input\n",
    "        self.conv1 = ConvLayer(3, 64)\n",
    "        \n",
    "        # Downscalling\n",
    "        self.down1 = DownScaller(64, 128)\n",
    "        self.down2 = DownScaller(128, 256)\n",
    "        self.down3 = DownScaller(256, 512)\n",
    "        \n",
    "        # Bridge\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(2, 2))   \n",
    "        self.up1 = UpScaller(512, 1024)\n",
    "        \n",
    "        # Upscalling\n",
    "        self.up2 = UpScaller(1024, 512)\n",
    "        self.up3 = UpScaller(512, 256)\n",
    "        self.up4 = UpScaller(256, 128)\n",
    "        \n",
    "        # Output\n",
    "        self.conv2 = ConvLayer(128, 64)\n",
    "        self.conv3 = nn.Conv2d(64, 1, kernel_size=(3, 3), stride=1, dilation=1)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def forward(self, input):\n",
    "        x_1 = self.conv1(input)\n",
    "        \n",
    "        x_2 = self.down1(x_1)\n",
    "        x_3 = self.down2(x_2)\n",
    "        x_4 = self.down3(x_3)\n",
    "        \n",
    "        x_5 = self.maxpool(x_4)\n",
    "        x_6 = self.up1(x_5)\n",
    "        \n",
    "        x_4_skip = self.center_crop(x_4, x_6)\n",
    "        x_7 = self.up2(x_4_skip)\n",
    "        \n",
    "        x_3_skip = self.center_crop(x_3, x_7)\n",
    "        x_8 = self.up3(x_3_skip)\n",
    "        \n",
    "        x_2_skip = self.center_crop(x_2, x_8)\n",
    "        x_9 = self.up4(x_2_skip)\n",
    "        \n",
    "        x_1_skip = self.center_crop(x_1, x_9)\n",
    "        x_10 = self.conv2(x_1_skip)\n",
    "        \n",
    "        output = self.conv3(x_10)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def center_crop(self, source_tensor, target_tensor):\n",
    "        (_, _, h, w) = target_tensor.shape\n",
    "        crop = CenterCrop((h, w))(source_tensor)\n",
    "        crop_concat = torch.cat([crop, target_tensor], dim=1)\n",
    "        \n",
    "        return crop_concat\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb1186a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK\n",
    "unet = UNet()\n",
    "a = torch.rand([4, 3, 572, 572])\n",
    "out = unet(a)\n",
    "\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52df12b9",
   "metadata": {},
   "source": [
    "### Train U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96531f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "model = UNet().to(device)\n",
    "summary(model, input_size=train_dataset[0]['image'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265290db",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963436f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "EPOCHS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04443b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=False, batch_size=BATCH_SIZE,\n",
    "                                           num_workers=12, persistent_workers=True, pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, shuffle=False, batch_size=BATCH_SIZE, \n",
    "                                          num_workers=12, persistent_workers=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bcbfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS), desc='EPOCH'):\n",
    "    \n",
    "    model.train(True)\n",
    "    loss_train = []\n",
    "    iou_train = []\n",
    "\n",
    "    for batch in tqdm(train_loader, total=len(train_dataset)):\n",
    "        x_batch, y_batch = batch['image'].to(device), batch['mask'].to(device)\n",
    "\n",
    "        output = model(x_batch)\n",
    "\n",
    "        loss = criterion(output, y_batch.squeeze(1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_train.append(loss.item())\n",
    "        iou_train.append(calc_iou(output, y_batch))\n",
    "    \n",
    "#     model.train(False)\n",
    "#     loss_test, iou_test = eval_model(model, test_loader, criterion)\n",
    "    \n",
    "    print(f'EPOCH {epoch + 1} / {EPOCHS}\\n'\n",
    "          f'Train loss: {torch.mean(loss_train):.3f}  iou: {torch.mean(iou_train):.3f}'\n",
    "#           f'Test loss: {torch.mean(loss_test):.3f}  iou: {torch.mean(iou_test):.3f}'\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac324fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
