{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "884a6408",
   "metadata": {},
   "source": [
    "# Seminar 11 - Stereo Depth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a2b4f6",
   "metadata": {},
   "source": [
    "This task is based on two papers:\n",
    "\n",
    "1) Mayer et al. \"A Large Dataset to Train Convolutional Networksfor Disparity, Optical Flow, and Scene Flow Estimation\", CVPR 2016, ([pdf](https://openaccess.thecvf.com/content_cvpr_2016/papers/Mayer_A_Large_Dataset_CVPR_2016_paper.pdf), [poster](https://lmb.informatik.uni-freiburg.de/Publications/2016/MIFDB16/poster-MIFDB16.pdf), [supplimentary materials](https://lmb.informatik.uni-freiburg.de/Publications/2016/MIFDB16/supplementary-MIFDB16.pdf), [project page](https://lmb.informatik.uni-freiburg.de/Publications/2016/MIFDB16/)) \n",
    "\n",
    "2) Fischer at al. \"FlowNet: Learning Optical Flow with Convolutional Networks\", ICCV 2015, [pdf](https://arxiv.org/pdf/1504.06852.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b81220",
   "metadata": {},
   "source": [
    "<img src=\"https://media.arxiv-vanity.com/render-output/4733381/images/monkaa/Cleanpass_0050_L.jpg\" style=\"width:60%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69d21a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!L\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from PIL import Image\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502eefea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!L\n",
    "def get_computing_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    return device\n",
    "\n",
    "device = get_computing_device()\n",
    "print(f\"Our main computing device is '{device}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956e3e12",
   "metadata": {},
   "source": [
    "## 0. Warm up\n",
    "\n",
    "1) What is rectified stereo pair?\n",
    "\n",
    "2) What is disparity?\n",
    "\n",
    "3) How does disparity help with depth computation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6855a328",
   "metadata": {},
   "source": [
    "## 1. KITTI Stereo Depth 2012\n",
    "\n",
    "http://www.cvlibs.net/datasets/kitti/eval_stereo_flow.php?benchmark=stereo\n",
    "\n",
    "194 training image pairs, 195 test image pairs with hidden ground truth, ground truth depth captured by lidar.\n",
    "\n",
    "Zero disparity value means that disparity is unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c02118a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gfile\n",
    "gfile.download_list(\n",
    "    'https://drive.google.com/file/d/12zitJCsOVmoCHII5Ym_t2AAORXb6WMyU',\n",
    "    filename='kitti_stereo_2012_training_data.zip',\n",
    "    target_dir='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d97f39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!unzip -q ./kitti_stereo_2012_training_data.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47c9624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_disparity(img):\n",
    "    img = img.astype(np.float32) / 256\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e27428b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_name = \"000002_10.png\"\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.title('left image')\n",
    "plt.imshow(Image.open(f'./kitti_stereo_2012_training_data/train/colored_0/{img_name}')); \n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.title('right image')\n",
    "plt.imshow(Image.open(f'./kitti_stereo_2012_training_data/train/colored_1/{img_name}')); \n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.title('disparity')\n",
    "disp = np.array(Image.open(f'./kitti_stereo_2012_training_data/train/disp_noc/{img_name}'))\n",
    "plt.imshow(normalize_disparity(disp), 'gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "           \n",
    "plt.subplot(2,2,4)\n",
    "plt.title('valid disparity mask')\n",
    "plt.imshow(disp > 0, 'gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5c392a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_max_disparity = normalize_disparity(disp).max()\n",
    "sample_shape = disp.shape\n",
    "\n",
    "print(f'max disp = {sample_max_disparity} , disp shape {sample_shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8241578b",
   "metadata": {},
   "source": [
    "## 2. Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39176731",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kitti_dataset import KITTIStereoRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57de9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "KITTIStereoRAM??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d5d07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "means = np.array([0.35715697, 0.37349922, 0.35886646] , dtype=np.float32)\n",
    "stds = np.array([0.27408948, 0.2807328,  0.27994434], dtype=np.float32)\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.ColorJitter(0.1, 0.1, 0.1, 0.1),\n",
    "    transforms.Normalize(means, stds),\n",
    "])\n",
    "\n",
    "# min kitti shape is [370, 1226], max shape is [376, 1242]\n",
    "PAD_HEIGHT = 128*3 \n",
    "PAD_WIDTH = 1280\n",
    "CROP_WIDTH = 768\n",
    "def transforms_train(left_image, right_image, disparity, valid_pixels_mask):\n",
    "    disparity = torchvision.transforms.functional.to_tensor(disparity)\n",
    "    valid_pixels_mask = torchvision.transforms.functional.to_tensor(valid_pixels_mask)\n",
    "    left_image = transform_train(left_image)\n",
    "    right_image = transform_train(right_image)\n",
    "    left_image = pad_to_size(left_image, PAD_HEIGHT, PAD_WIDTH)\n",
    "    right_image = pad_to_size(right_image, PAD_HEIGHT, PAD_WIDTH)\n",
    "    disparity = pad_to_size(disparity, PAD_HEIGHT, PAD_WIDTH)\n",
    "    valid_pixels_mask = pad_to_size(valid_pixels_mask, PAD_HEIGHT, PAD_WIDTH)\n",
    "\n",
    "    shift = torch.randint(0, PAD_WIDTH-CROP_WIDTH, (1,))\n",
    "    left_image = left_image[:,:,shift:shift+CROP_WIDTH]\n",
    "    right_image = right_image[:, :, shift: shift+CROP_WIDTH]\n",
    "    disparity = disparity[:, :, shift: shift+ CROP_WIDTH]\n",
    "    valid_pixels_mask = valid_pixels_mask[:, :, shift: shift+CROP_WIDTH]\n",
    "    return left_image, right_image, disparity, valid_pixels_mask\n",
    "\n",
    "def pad_to_size(images, min_height, min_width):\n",
    "    if images.shape[1] < min_height:\n",
    "        images = torchvision.transforms.functional.pad(images, (0,0,0,min_height-images.shape[1]))\n",
    "    if images.shape[2] < min_width:\n",
    "        images = torchvision.transforms.functional.pad(images, (0,0, min_width - images.shape[2], 0))\n",
    "    return images\n",
    "        \n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(means, stds),\n",
    "])\n",
    "\n",
    "def transforms_test(left_image, right_image, disparity, valid_pixels_mask):\n",
    "    disparity = torchvision.transforms.functional.to_tensor(disparity)\n",
    "    valid_pixels_mask = torchvision.transforms.functional.to_tensor(valid_pixels_mask)\n",
    "    left_image = transform_test(left_image)\n",
    "    right_image = transform_test(right_image)\n",
    "    left_image = pad_to_size(left_image, PAD_HEIGHT, PAD_WIDTH)\n",
    "    right_image = pad_to_size(right_image, PAD_HEIGHT, PAD_WIDTH)\n",
    "    disparity = pad_to_size(disparity, PAD_HEIGHT, PAD_WIDTH)\n",
    "    valid_pixels_mask = pad_to_size(valid_pixels_mask, PAD_HEIGHT, PAD_WIDTH)\n",
    "    \n",
    "    return left_image, right_image, disparity, valid_pixels_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940bad62",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = KITTIStereoRAM(root=\"./kitti_stereo_2012_training_data/\", train=True, transforms=transforms_train)\n",
    "\n",
    "train_batch_gen = torch.utils.data.DataLoader(train_loader, \n",
    "                                              batch_size=8,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=16)\n",
    "val_loader = KITTIStereoRAM(root=\"./kitti_stereo_2012_training_data/\", train=False, transforms=transforms_test)\n",
    "\n",
    "val_batch_gen = torch.utils.data.DataLoader(val_loader, \n",
    "                                              batch_size=1,\n",
    "                                              shuffle=False,\n",
    "                                              num_workers=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbef3c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in train_batch_gen:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55643d1",
   "metadata": {},
   "source": [
    "## 3. DispNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7395943",
   "metadata": {},
   "source": [
    "[Paper](https://openaccess.thecvf.com/content_cvpr_2016/papers/Mayer_A_Large_Dataset_CVPR_2016_paper.pdf), [poster](https://lmb.informatik.uni-freiburg.de/Publications/2016/MIFDB16/poster-MIFDB16.pdf), [supplimentary materials](https://lmb.informatik.uni-freiburg.de/Publications/2016/MIFDB16/supplementary-MIFDB16.pdf), [project page](https://lmb.informatik.uni-freiburg.de/Publications/2016/MIFDB16/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b61c29e",
   "metadata": {},
   "source": [
    "### 3.1 DispNet Simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcda6892",
   "metadata": {},
   "source": [
    "The simplest way to predict the disparity is just concat pair of images and feed it to unet-like architecture.\n",
    "\n",
    "[[FlowNet paper]](https://arxiv.org/pdf/1504.06852.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecae528",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/2400/0*LPtmtLr-mugr8OtN.png\" style=\"width:80%\">\n",
    "<img src=\"https://miro.medium.com/max/692/0*blFDiciN3KbPNeov.png\" style=\"width:80%\">\n",
    "\n",
    "Network architecture in more details:\n",
    "\n",
    "<img src=\"./dispnet.png\" style=\"width:50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c62d987",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBNRelu(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.conv = torch.nn.Conv2d(in_channels, out_channels, *args, **kwargs)\n",
    "        self.bn = torch.nn.BatchNorm2d(out_channels)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class UpConvBNRelu(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.conv = torch.nn.ConvTranspose2d(in_channels, out_channels, *args, **kwargs)\n",
    "        self.bn = torch.nn.BatchNorm2d(out_channels)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class DispNetSimple(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = ConvBNRelu(6, 64, kernel_size=(7,7), stride=2, padding=(3,3))\n",
    "        self.conv2 = ConvBNRelu(64, 128, kernel_size=(5,5), stride=2, padding=(2,2))\n",
    "        self.conv3 = torch.nn.Sequential(\n",
    "            ConvBNRelu(128, 256, kernel_size=(5,5), stride=2, padding=(2,2)),\n",
    "            ConvBNRelu(256, 256, kernel_size=(3,3), stride=1, padding=(1,1)))\n",
    "        self.conv4 = torch.nn.Sequential(\n",
    "            ConvBNRelu(256, 512, kernel_size=(3,3), stride=2, padding=(1,1)),\n",
    "            ConvBNRelu(512, 512, kernel_size=(3,3), stride=1, padding=(1,1)))\n",
    "        self.conv5 = torch.nn.Sequential(\n",
    "            ConvBNRelu(512, 512, kernel_size=(3,3), stride=2, padding=(1,1)),\n",
    "            ConvBNRelu(512, 512, kernel_size=(3,3), stride=1, padding=(1,1)))\n",
    "        self.conv6 = torch.nn.Sequential(\n",
    "            ConvBNRelu(512, 1024, kernel_size=(3,3), stride=2, padding=(1,1)),\n",
    "            ConvBNRelu(1024, 1024, kernel_size=(3,3), stride=1, padding=(1,1)))\n",
    "        self.pred6 = torch.nn.Conv2d(1024, 1, kernel_size=3, stride=1, padding=(1,1))\n",
    "        \n",
    "        self.upconv5 = UpConvBNRelu(1024, 512, kernel_size=4, stride=2, padding=(1,1))\n",
    "        self.iconv5 = ConvBNRelu(1025, 512, kernel_size=3, stride=1, padding=(1,1))\n",
    "        self.pred5 = torch.nn.Conv2d(512, 1, kernel_size=3, stride=1, padding=(1,1))\n",
    "\n",
    "        self.upconv4 = UpConvBNRelu(512, 256, kernel_size=4, stride=2, padding=(1,1))\n",
    "        self.iconv4 = ConvBNRelu(256+512+1, 256, kernel_size=3, stride=1, padding=(1,1))\n",
    "        self.pred4 = torch.nn.Conv2d(256, 1, kernel_size=3, stride=1, padding=(1,1))\n",
    "        \n",
    "        self.upconv3 = UpConvBNRelu(256, 128, kernel_size=4, stride=2, padding=(1,1))\n",
    "        self.iconv3 = ConvBNRelu(128+256+1, 128, kernel_size=3, stride=1, padding=(1,1))\n",
    "        self.pred3 = torch.nn.Conv2d(128, 1, kernel_size=3, stride=1, padding=(1,1))\n",
    "\n",
    "        self.upconv2 = UpConvBNRelu(128, 64, kernel_size=4, stride=2, padding=(1,1))\n",
    "        self.iconv2 = ConvBNRelu(64+128+1, 64, kernel_size=3, stride=1, padding=(1,1))\n",
    "        self.pred2 = torch.nn.Conv2d(64, 1, kernel_size=3, stride=1, padding=(1,1))\n",
    "\n",
    "        self.upconv1 = UpConvBNRelu(64, 32, kernel_size=4, stride=2, padding=(1,1))\n",
    "        self.iconv1 = ConvBNRelu(32+64+1, 32, kernel_size=3, stride=1, padding=(1,1))\n",
    "        self.pred1 = torch.nn.Conv2d(32, 1, kernel_size=3, stride=1, padding=(1,1))\n",
    "        \n",
    "    def forward(self, left_img, right_img):\n",
    "        x = torch.cat([left_img, right_img], dim=1)\n",
    "        \n",
    "        # TODO apply dispnet        \n",
    "        return predictions_per_scale\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb045ff",
   "metadata": {},
   "source": [
    "Let's check that it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0133587",
   "metadata": {},
   "outputs": [],
   "source": [
    "dispnet = DispNetSimple()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36373925",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in train_batch_gen:\n",
    "    left, right, target, mask = sample\n",
    "    res = dispnet(left, right)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaaa49a",
   "metadata": {},
   "source": [
    "### 3.2 Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1ae3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(predicted, target, mask):\n",
    "    losses = []\n",
    "    target_masked = target[mask]\n",
    "    for scale_pred in predicted:\n",
    "        scale_pred = torch.nn.functional.interpolate(\n",
    "            scale_pred, size=target.shape[-2:], mode='bilinear', align_corners=True)\n",
    "        scale_pred = scale_pred[mask]\n",
    "        losses.append(torch.nn.functional.huber_loss(scale_pred, target_masked))\n",
    "    total_loss = sum(losses) / len(losses)\n",
    "    return total_loss, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd90a840",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_loss(res, target, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b921fc",
   "metadata": {},
   "source": [
    "### 3.3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950189eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "dispnet = DispNetSimple()\n",
    "dispnet = dispnet.to(device)\n",
    "\n",
    "opt = torch.optim.Adam(dispnet.parameters(), lr=1e-3, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcee6166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(network, opt, num_epochs=20):\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        train_loss = []\n",
    "        val_loss = []\n",
    "        train_scale_losses = []\n",
    "        val_scale_losses = []\n",
    "\n",
    "        network.train(True)\n",
    "\n",
    "        for x_left, x_right, gt, valid_pixels_mask in tqdm.tqdm(train_batch_gen):\n",
    "            opt.zero_grad()\n",
    "            x_left = x_left.to(device)\n",
    "            x_right = x_right.to(device)\n",
    "            valid_pvalid_pixels_mask_mask = valid_pixels_mask.to(device)\n",
    "            gt = gt.to(device)\n",
    "\n",
    "            pred = network(x_left, x_right)\n",
    "\n",
    "            loss, scale_losses = compute_loss(pred, gt, valid_pixels_mask)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            train_loss.append(loss.cpu().data.numpy())\n",
    "            train_scale_losses.append(np.array([elem.cpu().data.numpy() for elem in scale_losses]))\n",
    "\n",
    "        network.train(False)\n",
    "        with torch.no_grad():\n",
    "            for x_left, x_right, gt, valid_pixels_mask in val_batch_gen:\n",
    "                x_left = x_left.to(device)\n",
    "                x_right = x_right.to(device)\n",
    "                gt = gt.to(device)\n",
    "                valid_pixels_mask = valid_pixels_mask.to(device)\n",
    "\n",
    "                pred = network(x_left, x_right)\n",
    "\n",
    "                loss, scale_losses = compute_loss(pred, gt, valid_pixels_mask)\n",
    "\n",
    "                val_loss.append(loss.cpu().data.numpy())\n",
    "                val_scale_losses.append(np.array([elem.cpu().data.numpy() for elem in scale_losses]))\n",
    "\n",
    "        # Then we print the results for this epoch:\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss (in-iteration): \\t{:.6f} , \\t component loss: {}\".format(\n",
    "            np.mean(train_loss), np.mean(np.stack(train_scale_losses), axis=0)))\n",
    "        print(\"  validation loss: \\t\\t\\t{:.2f} , \\t\\t componnet loss: {}\".format(\n",
    "            np.mean(val_loss), np.mean(np.stack(val_scale_losses), axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1e367d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_network(dispnet, opt, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a70b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_result(network, img_index):\n",
    "    network.train(False)\n",
    "    for i, (x_left, x_right, target, mask) in enumerate(val_batch_gen):\n",
    "        if i != img_index:\n",
    "            continue\n",
    "        pred = network(x_left.to(device), x_right.to(device))\n",
    "        pred = pred[-1].cpu()\n",
    "        break\n",
    "        \n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.subplot(3,1,1)\n",
    "    plt.title('left image')\n",
    "    plt.imshow(val_loader.left_images[img_index])\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    plt.subplot(3,1,2)\n",
    "    plt.title('gt')\n",
    "    plt.imshow(val_loader.targets[img_index])\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    plt.subplot(3,1,3)\n",
    "    plt.title('pred')\n",
    "    plt.imshow(pred.data.numpy()[0,0])\n",
    "    plt.xticks([]), plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c454040",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_result(dispnet, img_index=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0844e2ba",
   "metadata": {},
   "source": [
    "### 3.4 DispNet-Corr1D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e84af05",
   "metadata": {},
   "source": [
    "(image is taken from [poster](https://lmb.informatik.uni-freiburg.de/Publications/2016/MIFDB16/poster-MIFDB16.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b56fb0",
   "metadata": {},
   "source": [
    "<img src=\"./dispnet-corr1d.png\" style=\"width:80%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525db8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corr1DLayer(torch.nn.Module):\n",
    "    def __init__(self, max_disp):\n",
    "        super().__init__()\n",
    "        self.max_disp = max_disp\n",
    "\n",
    "    def forward(self, left_img, right_img):\n",
    "        corr_result = []\n",
    "        for shift in range(0, self.max_disp):\n",
    "            # YOUR CODE\n",
    "            ...\n",
    "            corr = ...\n",
    "            corr_result.append(corr)\n",
    "        corr_result = torch.stack(corr_result, dim=1)\n",
    "        return corr_result\n",
    "        \n",
    "        \n",
    "class DispNetCorr1D(torch.nn.Module):\n",
    "    def __init__(self, max_disp=40):\n",
    "        super().__init__()\n",
    "        self.conv1 = ConvBNRelu(3, 64, kernel_size=(7,7), stride=2, padding=(3,3))\n",
    "        self.conv2 = ConvBNRelu(64, 128, kernel_size=(5,5), stride=2, padding=(2,2))\n",
    "        \n",
    "        self.corr1d = Corr1DLayer(max_disp)\n",
    "        self.conv_refinement = ConvBNRelu(128, 64, kernel_size=(3,3), stride=1, padding=(1,1))\n",
    "        \n",
    "        self.conv3 = torch.nn.Sequential(\n",
    "            ConvBNRelu(64+max_disp, 256, kernel_size=(5,5), stride=2, padding=(2,2)),\n",
    "            ConvBNRelu(256, 256, kernel_size=(3,3), stride=1, padding=(1,1)))\n",
    "        self.conv4 = torch.nn.Sequential(\n",
    "            ConvBNRelu(256, 512, kernel_size=(3,3), stride=2, padding=(1,1)),\n",
    "            ConvBNRelu(512, 512, kernel_size=(3,3), stride=1, padding=(1,1)))\n",
    "        self.conv5 = torch.nn.Sequential(\n",
    "            ConvBNRelu(512, 512, kernel_size=(3,3), stride=2, padding=(1,1)),\n",
    "            ConvBNRelu(512, 512, kernel_size=(3,3), stride=1, padding=(1,1)))\n",
    "        self.conv6 = torch.nn.Sequential(\n",
    "            ConvBNRelu(512, 1024, kernel_size=(3,3), stride=2, padding=(1,1)),\n",
    "            ConvBNRelu(1024, 1024, kernel_size=(3,3), stride=1, padding=(1,1)))\n",
    "        self.pred6 = torch.nn.Conv2d(1024, 1, kernel_size=3, stride=1, padding=(1,1))\n",
    "        \n",
    "        self.upconv5 = UpConvBNRelu(1024, 512, kernel_size=4, stride=2, padding=(1,1))\n",
    "        self.iconv5 = ConvBNRelu(1025, 512, kernel_size=3, stride=1, padding=(1,1))\n",
    "        self.pred5 = torch.nn.Conv2d(512, 1, kernel_size=3, stride=1, padding=(1,1))\n",
    "\n",
    "        self.upconv4 = UpConvBNRelu(512, 256, kernel_size=4, stride=2, padding=(1,1))\n",
    "        self.iconv4 = ConvBNRelu(256+512+1, 256, kernel_size=3, stride=1, padding=(1,1))\n",
    "        self.pred4 = torch.nn.Conv2d(256, 1, kernel_size=3, stride=1, padding=(1,1))\n",
    "        \n",
    "        self.upconv3 = UpConvBNRelu(256, 128, kernel_size=4, stride=2, padding=(1,1))\n",
    "        self.iconv3 = ConvBNRelu(128+256+1, 128, kernel_size=3, stride=1, padding=(1,1))\n",
    "        self.pred3 = torch.nn.Conv2d(128, 1, kernel_size=3, stride=1, padding=(1,1))\n",
    "\n",
    "        self.upconv2 = UpConvBNRelu(128, 64, kernel_size=4, stride=2, padding=(1,1))\n",
    "        self.iconv2 = ConvBNRelu(64+128+1, 64, kernel_size=3, stride=1, padding=(1,1))\n",
    "        self.pred2 = torch.nn.Conv2d(64, 1, kernel_size=3, stride=1, padding=(1,1))\n",
    "\n",
    "        self.upconv1 = UpConvBNRelu(64, 32, kernel_size=4, stride=2, padding=(1,1))\n",
    "        self.iconv1 = ConvBNRelu(32+64+1, 32, kernel_size=3, stride=1, padding=(1,1))\n",
    "        self.pred1 = torch.nn.Conv2d(32, 1, kernel_size=3, stride=1, padding=(1,1))\n",
    "        \n",
    "    def forward(self, left_img, right_img):\n",
    "        \n",
    "        # YOUR CODE\n",
    "        \n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6958b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dispnet = DispNetCorr1D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdbbdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in train_batch_gen:\n",
    "    left, right, target, mask = sample\n",
    "    res = dispnet(left, right)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d54e1e",
   "metadata": {},
   "source": [
    "### 3.5 DispNet-Corr1D training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798ed068",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "dispnet = DispNetCorr1D(max_disp=40)\n",
    "dispnet = dispnet.to(device)\n",
    "\n",
    "opt = torch.optim.Adam(dispnet.parameters(), lr=1e-3, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be5aac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_network(dispnet, opt, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3283aeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_result(dispnet, img_index=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
