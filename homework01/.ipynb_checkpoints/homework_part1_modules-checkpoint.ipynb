{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-17T10:53:49.664986Z",
     "start_time": "2022-04-17T10:53:49.515346Z"
    },
    "id": "UrkSsCyT5CoR"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ObuqtNmF5CoW"
   },
   "source": [
    "**Module** is an abstract class which defines fundamental methods necessary for a training a neural network. You do not need to change anything here, just read the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-17T10:53:50.170928Z",
     "start_time": "2022-04-17T10:53:50.151196Z"
    },
    "id": "9-RImaVZ5CoW"
   },
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    \"\"\"\n",
    "    Basically, you can think of a module as of a something (black box) \n",
    "    which can process `input` data and produce `ouput` data.\n",
    "    This is like applying a function which is called `forward`: \n",
    "        \n",
    "        output = module.forward(input)\n",
    "    \n",
    "    The module should be able to perform a backward pass: to differentiate the `forward` function. \n",
    "    More, it should be able to differentiate it if is a part of chain (chain rule).\n",
    "    The latter implies there is a gradient from previous step of a chain rule. \n",
    "    \n",
    "        input_grad = module.backward(input, output_grad)\n",
    "    \"\"\"\n",
    "    def __init__ (self):\n",
    "        self._output = None\n",
    "        self._input_grad = None\n",
    "        self.training = True\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Takes an input object, and computes the corresponding output of the module.\n",
    "        \"\"\"\n",
    "        self._output = self._compute_output(input)\n",
    "        return self._output\n",
    "\n",
    "    def backward(self, input, output_grad):\n",
    "        \"\"\"\n",
    "        Performs a backpropagation step through the module, with respect to the given input.\n",
    "        \n",
    "        This includes \n",
    "         - computing a gradient w.r.t. `input` (is needed for further backprop),\n",
    "         - computing a gradient w.r.t. parameters (to update parameters while optimizing).\n",
    "        \"\"\"\n",
    "        self._input_grad = self._compute_input_grad(input, output_grad)\n",
    "        self._update_parameters_grad(input, output_grad)\n",
    "        return self._input_grad\n",
    "    \n",
    "\n",
    "    def _compute_output(self, input):\n",
    "        \"\"\"\n",
    "        Computes the output using the current parameter set of the class and input.\n",
    "        This function returns the result which will be stored in the `_output` field.\n",
    "\n",
    "        Example: in case of identity operation:\n",
    "        \n",
    "        output = input \n",
    "        return output\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "\n",
    "    def _compute_input_grad(self, input, output_grad):\n",
    "        \"\"\"\n",
    "        Returns the gradient of the module with respect to its own input. \n",
    "        The shape of the returned value is always the same as the shape of `input`.\n",
    "        \n",
    "        Example: in case of identity operation:\n",
    "        input_grad = output_grad\n",
    "        return input_grad\n",
    "        \"\"\"\n",
    "        \n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def _update_parameters_grad(self, input, output_grad):\n",
    "        \"\"\"\n",
    "        Computing the gradient of the module with respect to its own parameters.\n",
    "        No need to override if module has no parameters (e.g. ReLU).\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def zero_grad(self): \n",
    "        \"\"\"\n",
    "        Zeroes `gradParams` variable if the module has params.\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "    def get_parameters(self):\n",
    "        \"\"\"\n",
    "        Returns a list with its parameters. \n",
    "        If the module does not have parameters return empty list. \n",
    "        \"\"\"\n",
    "        return []\n",
    "        \n",
    "    def get_parameters_grad(self):\n",
    "        \"\"\"\n",
    "        Returns a list with gradients with respect to its parameters. \n",
    "        If the module does not have parameters return empty list. \n",
    "        \"\"\"\n",
    "        return []\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Sets training mode for the module.\n",
    "        Training and testing behaviour differs for Dropout, BatchNorm.\n",
    "        \"\"\"\n",
    "        self.training = True\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Sets evaluation mode for the module.\n",
    "        Training and testing behaviour differs for Dropout, BatchNorm.\n",
    "        \"\"\"\n",
    "        self.training = False\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Pretty printing. Should be overrided in every module if you want \n",
    "        to have readable description. \n",
    "        \"\"\"\n",
    "        return \"Module\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3bZIgerk5Cob"
   },
   "source": [
    "# Sequential container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YLxKoVrm5Cod"
   },
   "source": [
    "**Define** a forward and backward pass procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "7Hs_eDpe5Coe"
   },
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    \"\"\"\n",
    "         This class implements a container, which processes `input` data sequentially. \n",
    "         \n",
    "         `input` is processed by each module (layer) in self.modules consecutively.\n",
    "         The resulting array is called `_output`. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__ (self):\n",
    "        super(Sequential, self).__init__()\n",
    "        self.modules = []\n",
    "   \n",
    "    def add_module(self, module):\n",
    "        \"\"\"\n",
    "        Adds a module to the container.\n",
    "        \"\"\"\n",
    "        self.modules.append(module)\n",
    "\n",
    "    def _compute_output(self, input):\n",
    "        \"\"\"\n",
    "        Basic workflow of FORWARD PASS:\n",
    "        \n",
    "            y_0    = module[0].forward(input)\n",
    "            y_1    = module[1].forward(y_0)\n",
    "            ...\n",
    "            output = module[n-1].forward(y_{n-2})   \n",
    "            \n",
    "            \n",
    "        Just write a little loop. \n",
    "        \"\"\"\n",
    "\n",
    "        # Your code goes here. ################################################\n",
    "        \n",
    "        output = input\n",
    "        \n",
    "        for module in self.modules:\n",
    "            output = module.forward(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def _compute_input_grad(self, input, output_grad):\n",
    "        \"\"\"\n",
    "        Workflow of BACKWARD PASS:\n",
    "            \n",
    "            g_{n-1} = module[n-1].backward(y_{n-2}, output_grad)\n",
    "            g_{n-2} = module[n-2].backward(y_{n-3}, g_{n-1})\n",
    "            ...\n",
    "            g_1 = module[1].backward(y_0, g_2)   \n",
    "            grad_input = module[0].backward(input, g_1)   \n",
    "             \n",
    "             \n",
    "        !!!\n",
    "                \n",
    "        To each module you need to provide the input, module saw while forward pass, \n",
    "        it is used while computing gradients. \n",
    "        Make sure that the input for `i-th` layer the output of `module[i]` (just the same input as in forward pass) \n",
    "        and NOT `input` to this Sequential module. \n",
    "        \n",
    "        !!!\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        # Your code goes here. ################################################\n",
    "        layers_count = len(self.modules)\n",
    "                \n",
    "        for i in reversed(range(1, layers_count)):\n",
    "            print(self.modules[i], self.modules[i - 1])\n",
    "            output_grad = self.modules[i].backward(self.modules[i - 1]._output, output_grad)\n",
    "            \n",
    "        grad_input = self.modules[0].backward(input, output_grad)\n",
    "\n",
    "        return grad_input\n",
    "      \n",
    "\n",
    "    def zero_grad(self): \n",
    "        for module in self.modules:\n",
    "            module.zero_grad()\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        \"\"\"\n",
    "        Should gather all parameters in a list.\n",
    "        \"\"\"\n",
    "        return [x.get_parameters() for x in self.modules]\n",
    "    \n",
    "    def get_parameters_grad(self):\n",
    "        \"\"\"\n",
    "        Should gather all gradients w.r.t parameters in a list.\n",
    "        \"\"\"\n",
    "        return [x.get_parameters_grad() for x in self.modules]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        string = \"\".join([str(x) + '\\n' for x in self.modules])\n",
    "        return string\n",
    "    \n",
    "    def __getitem__(self, x):\n",
    "        return self.modules.__getitem__(x)\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Propagates training parameter through all modules\n",
    "        \"\"\"\n",
    "        self.training = True\n",
    "        for module in self.modules:\n",
    "            module.train()\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Propagates training parameter through all modules\n",
    "        \"\"\"\n",
    "        self.training = False\n",
    "        for module in self.modules:\n",
    "            module.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_BatchNormalization (__main__.TestLayers) ... ok\n",
      "test_ClassNLLCriterion (__main__.TestLayers) ... ERROR\n",
      "test_ClassNLLCriterionUnstable (__main__.TestLayers) ... ERROR\n",
      "test_Dropout (__main__.TestLayers) ... ok\n",
      "test_ELU (__main__.TestLayers) ... ok\n",
      "test_LeakyReLU (__main__.TestLayers) ... ok\n",
      "test_Linear (__main__.TestLayers) ... ok\n",
      "test_LogSoftMax (__main__.TestLayers) ... ok\n",
      "test_Sequential (__main__.TestLayers) ... FAIL\n",
      "test_SoftMax (__main__.TestLayers) ... ok\n",
      "test_SoftPlus (__main__.TestLayers) ... ok\n",
      "test_adam_optimizer (__main__.TestLayers) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChannelwiseScaling BatchNormalization\n",
      "[[ 4.2291183e-05 -8.4645563e-04  2.3606012e-04 -4.6940963e-03]\n",
      " [-4.2291183e-05  8.4645563e-04 -2.3601248e-04  4.6940963e-03]] (2, 4) \n",
      "\n",
      "[[ 4.22756446e-05 -8.46460167e-04  2.36158235e-04 -4.69414524e-03]\n",
      " [-4.22756446e-05  8.46460167e-04 -2.36158235e-04  4.69414524e-03]] (2, 4)\n",
      "\n",
      "\n",
      "ChannelwiseScaling BatchNormalization\n",
      "[[-6.1255819e-03 -6.0469951e-03  6.9653260e-04 -9.9579986e-05]\n",
      " [ 6.1252336e-03  6.0469951e-03 -6.9636072e-04  9.9457502e-05]] (2, 4) \n",
      "\n",
      "[[-6.12524694e-03 -6.04687277e-03  6.96530918e-04 -9.94825485e-05]\n",
      " [ 6.12524694e-03  6.04687277e-03 -6.96530918e-04  9.94825485e-05]] (2, 4)\n",
      "\n",
      "\n",
      "ChannelwiseScaling BatchNormalization\n",
      "[[-1.1022847e-02 -4.3516336e-03  3.8878056e-03  1.3936582e-05]\n",
      " [ 1.1021730e-02  4.3524676e-03 -3.8878964e-03 -1.4026496e-05]] (2, 4) \n",
      "\n",
      "[[-1.10287856e-02 -4.35231525e-03  3.88966179e-03  1.40285939e-05]\n",
      " [ 1.10287856e-02  4.35231525e-03 -3.88966179e-03 -1.40285939e-05]] (2, 4)\n",
      "\n",
      "\n",
      "ChannelwiseScaling BatchNormalization\n",
      "[[ 0.00022874  0.00075542  0.00019989  0.00036935]\n",
      " [-0.00022874 -0.00075542 -0.00019989 -0.00036959]] (2, 4) \n",
      "\n",
      "[[ 0.00022889  0.00075536  0.00019989  0.00036943]\n",
      " [-0.00022889 -0.00075536 -0.00019989 -0.00036943]] (2, 4)\n",
      "\n",
      "\n",
      "ChannelwiseScaling BatchNormalization\n",
      "[[-1.4766252e-03 -2.4767555e-02  3.6025778e-04 -9.9600205e-05]\n",
      " [ 1.4766252e-03  2.4767555e-02 -3.6025778e-04  9.9600205e-05]] (2, 4) \n",
      "\n",
      "[[-1.47686695e-03 -2.47676178e-02  3.60280868e-04 -9.95923305e-05]\n",
      " [ 1.47686695e-03  2.47676178e-02 -3.60280868e-04  9.95923305e-05]] (2, 4)\n",
      "\n",
      "\n",
      "ChannelwiseScaling BatchNormalization\n",
      "[[ 7.5692951e-05 -4.0250164e-04  2.9960103e-05  1.2156252e-04]\n",
      " [-7.5647731e-05  4.0258921e-04 -2.9960103e-05 -1.2156252e-04]] (2, 4) \n",
      "\n",
      "[[ 7.57167727e-05 -4.02431085e-04  2.98870457e-05  1.21549543e-04]\n",
      " [-7.57167727e-05  4.02431085e-04 -2.98870457e-05 -1.21549543e-04]] (2, 4)\n",
      "\n",
      "\n",
      "ChannelwiseScaling BatchNormalization\n",
      "[[-2.5598722e-02 -9.5926305e-05 -7.8822225e-03  6.5487291e-04]\n",
      " [ 2.5597293e-02  9.5926305e-05  7.8831082e-03 -6.5487291e-04]] (2, 4) \n",
      "\n",
      "[[-2.55982761e-02 -9.58584888e-05 -7.88378581e-03  6.55339397e-04]\n",
      " [ 2.55982761e-02  9.58584888e-05  7.88378581e-03 -6.55339397e-04]] (2, 4)\n",
      "\n",
      "\n",
      "ChannelwiseScaling BatchNormalization\n",
      "[[-4.9126127e-05  2.7476093e-02  5.0821151e-03  2.0491754e-04]\n",
      " [ 4.9126127e-05 -2.7476465e-02 -5.0815321e-03 -2.0491754e-04]] (2, 4) \n",
      "\n",
      "[[-4.90799158e-05  2.74765059e-02  5.08169914e-03  2.04888058e-04]\n",
      " [ 4.90799158e-05 -2.74765059e-02 -5.08169914e-03 -2.04888058e-04]] (2, 4)\n",
      "\n",
      "\n",
      "ChannelwiseScaling BatchNormalization\n",
      "[[-1.3212618e-04  3.7828586e-06  2.7335295e-04  3.2285915e-03]\n",
      " [ 1.3212618e-04 -3.7828586e-06 -2.7335295e-04 -3.2278509e-03]] (2, 4) \n",
      "\n",
      "[[-1.32090331e-04  3.74948159e-06  2.73396127e-04  3.22817772e-03]\n",
      " [ 1.32090331e-04 -3.74948159e-06 -2.73396127e-04 -3.22817772e-03]] (2, 4)\n",
      "\n",
      "\n",
      "ChannelwiseScaling BatchNormalization\n",
      "[[ 1.9875099e-05  1.1776416e-04 -5.3518661e-03 -9.3927840e-03]\n",
      " [-1.9863590e-05 -1.1765754e-04  5.3517600e-03  9.3914568e-03]] (2, 4) \n",
      "\n",
      "[[ 1.98022542e-05  1.17734643e-04 -5.35194194e-03 -9.39243085e-03]\n",
      " [-1.98022542e-05 -1.17734643e-04  5.35194194e-03  9.39243085e-03]] (2, 4)\n",
      "\n",
      "\n",
      "ChannelwiseScaling BatchNormalization\n",
      "[[-1.8134379e-01 -5.8913319e-03  7.9784140e-06  1.5081855e-04]\n",
      " [ 1.8134379e-01  5.8923219e-03 -7.9844240e-06 -1.5081855e-04]] (2, 4) \n",
      "\n",
      "[[-1.81344606e-01 -5.89186573e-03  7.98330983e-06  1.50790451e-04]\n",
      " [ 1.81344606e-01  5.89186573e-03 -7.98330983e-06 -1.50790451e-04]] (2, 4)\n",
      "\n",
      "\n",
      "ChannelwiseScaling BatchNormalization\n",
      "[[ 2.4283337e-04  3.7415609e-01  1.0401121e-02  7.8470109e-04]\n",
      " [-2.4283337e-04 -3.7414572e-01 -1.0401346e-02 -7.8470109e-04]] (2, 4) \n",
      "\n",
      "[[ 2.42750607e-04  3.74176215e-01  1.04008176e-02  7.84939311e-04]\n",
      " [-2.42750607e-04 -3.74176215e-01 -1.04008176e-02 -7.84939311e-04]] (2, 4)\n",
      "\n",
      "\n",
      "ChannelwiseScaling BatchNormalization\n",
      "[[-6.30381808e-04 -2.31739996e-05  2.38190565e-04  1.21608922e-04]\n",
      " [ 6.30465685e-04  2.32126222e-05 -2.38080349e-04 -1.21622834e-04]] (2, 4) \n",
      "\n",
      "[[-6.30535254e-04 -2.31829329e-05  2.38114412e-04  1.21582910e-04]\n",
      " [ 6.30535254e-04  2.31829329e-05 -2.38114412e-04 -1.21582910e-04]] (2, 4)\n",
      "\n",
      "\n",
      "ChannelwiseScaling BatchNormalization\n",
      "[[-1.6362329e-06 -5.0002785e-07 -4.4299862e-03 -4.4509543e-05]\n",
      " [ 1.5706196e-06  4.2922744e-07  4.4289585e-03  4.4509543e-05]] (2, 4) \n",
      "\n",
      "[[-1.61987470e-06 -4.39132424e-07 -4.43009893e-03 -4.45124690e-05]\n",
      " [ 1.61987470e-06  4.39132424e-07  4.43009893e-03  4.45124690e-05]] (2, 4)\n",
      "\n",
      "\n",
      "ChannelwiseScaling BatchNormalization\n",
      "[[ 1.3123138e-04  1.8853845e-05 -4.7090496e-03 -1.4671981e-05]\n",
      " [-1.3123138e-04 -1.8853845e-05  4.7090496e-03  1.4646597e-05]] (2, 4) \n",
      "\n",
      "[[ 1.31100283e-04  1.88205523e-05 -4.70903959e-03 -1.46721781e-05]\n",
      " [-1.31100283e-04 -1.88205523e-05  4.70903959e-03  1.46721781e-05]] (2, 4)\n",
      "\n",
      "\n",
      "ChannelwiseScaling BatchNormalization\n",
      "[[-1.79373907e-04  4.24645759e-06 -1.01626334e-04 -6.42167288e-05]\n",
      " [ 1.79373907e-04 -4.24645759e-06  1.01626334e-04  6.42167288e-05]] (2, 4) \n",
      "\n",
      "[[-1.79392438e-04  4.20800392e-06 -1.01600980e-04 -6.43449558e-05]\n",
      " [ 1.79392438e-04 -4.20800392e-06  1.01600980e-04  6.43449558e-05]] (2, 4)\n",
      "\n",
      "\n",
      "ChannelwiseScaling BatchNormalization\n",
      "[[ 1.1939701e-05  1.3962136e-04 -2.1119053e-04 -3.9934839e-05]\n",
      " [-1.1996153e-05 -1.3944760e-04  2.1129411e-04  3.9940063e-05]] (2, 4) \n",
      "\n",
      "[[ 1.19762760e-05  1.39527302e-04 -2.11343660e-04 -3.97649785e-05]\n",
      " [-1.19762760e-05 -1.39527302e-04  2.11343660e-04  3.97649785e-05]] (2, 4)\n",
      "\n",
      "\n",
      "ChannelwiseScaling BatchNormalization\n",
      "[[-1.4966252e-05 -5.2833180e-05  2.4835516e+01  3.6291435e-04]\n",
      " [ 1.4983963e-05  5.2901087e-05 -2.4835489e+01 -3.6273565e-04]] (2, 4) \n",
      "\n",
      "[[-1.50219171e-05 -5.29589669e-05  2.48354973e+01  3.62918545e-04]\n",
      " [ 1.50219171e-05  5.29589669e-05 -2.48354973e+01 -3.62918545e-04]] (2, 4)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_ClassNLLCriterion (__main__.TestLayers)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\manos\\AppData\\Local\\Temp/ipykernel_23168/310261970.py\", line 369, in test_ClassNLLCriterion\n",
      "    torch_layer_output_var = torch_layer(layer_input_var,\n",
      "  File \"C:\\Users\\manos\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"C:\\Users\\manos\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\loss.py\", line 211, in forward\n",
      "    return F.nll_loss(input, target, weight=self.weight, ignore_index=self.ignore_index, reduction=self.reduction)\n",
      "  File \"C:\\Users\\manos\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\functional.py\", line 2532, in nll_loss\n",
      "    return torch._C._nn.nll_loss_nd(input, target, weight, _Reduction.get_enum(reduction), ignore_index)\n",
      "RuntimeError: expected scalar type Long but found Int\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_ClassNLLCriterionUnstable (__main__.TestLayers)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\manos\\AppData\\Local\\Temp/ipykernel_23168/310261970.py\", line 340, in test_ClassNLLCriterionUnstable\n",
      "    torch_layer_output_var = torch_layer(torch.log(layer_input_var),\n",
      "  File \"C:\\Users\\manos\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"C:\\Users\\manos\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\loss.py\", line 211, in forward\n",
      "    return F.nll_loss(input, target, weight=self.weight, ignore_index=self.ignore_index, reduction=self.reduction)\n",
      "  File \"C:\\Users\\manos\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\functional.py\", line 2532, in nll_loss\n",
      "    return torch._C._nn.nll_loss_nd(input, target, weight, _Reduction.get_enum(reduction), ignore_index)\n",
      "RuntimeError: expected scalar type Long but found Int\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_adam_optimizer (__main__.TestLayers)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\manos\\AppData\\Local\\Temp/ipykernel_23168/310261970.py\", line 394, in test_adam_optimizer\n",
      "    opt = Adam(net, lr=1e-3, betas=(0.9, 0.999), eps=1e-8)\n",
      "NameError: name 'Adam' is not defined\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_Sequential (__main__.TestLayers)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\manos\\AppData\\Local\\Temp/ipykernel_23168/310261970.py\", line 165, in test_Sequential\n",
      "    self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n",
      "AssertionError: False is not true\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 12 tests in 0.869s\n",
      "\n",
      "FAILED (failures=1, errors=3)\n"
     ]
    }
   ],
   "source": [
    "%run homework_part1_test_modules.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "isbDKqDW5Col"
   },
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFnN0hb-5Col"
   },
   "source": [
    "## 1. Linear transform layer\n",
    "Also known as dense layer, fully-connected layer, FC-layer, InnerProductLayer (in caffe), affine transform\n",
    "- input:   **`batch_size x n_feats1`**\n",
    "- output: **`batch_size x n_feats2`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-17T11:24:54.260442Z",
     "start_time": "2022-04-17T11:24:54.241820Z"
    },
    "id": "Mgx-6sK65Com"
   },
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    \"\"\"\n",
    "    A module which applies a linear transformation \n",
    "    A common name is fully-connected layer, InnerProductLayer in caffe. \n",
    "    \n",
    "    The module should work with 2D input of shape (n_samples, n_feature).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super(Linear, self).__init__()\n",
    "       \n",
    "        # This is a nice initialization\n",
    "        stdv = 1. / np.sqrt(n_in)\n",
    "        self.W = np.random.uniform(-stdv, stdv, size=(n_out, n_in))\n",
    "        self.b = np.random.uniform(-stdv, stdv, size=n_out)\n",
    "        \n",
    "        self.gradW = np.zeros_like(self.W)\n",
    "        self.gradb = np.zeros_like(self.b)\n",
    "        \n",
    "    def _compute_output(self, input):\n",
    "        # Your code goes here. ################################################\n",
    "        output = np.matmul(input, self.W.T) + self.b\n",
    "        return output\n",
    "    \n",
    "    def _compute_input_grad(self, input, output_grad):\n",
    "        # Your code goes here. ################################################\n",
    "        grad_input = np.matmul(output_grad, self.W)\n",
    "        return grad_input\n",
    "    \n",
    "    def _update_parameters_grad(self, input, output_grad):\n",
    "        # Your code goes here. ################################################\n",
    "        self.gradW = np.matmul(output_grad.T, input)\n",
    "        self.gradb = np.sum(output_grad, axis=0)\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.gradW.fill(0)\n",
    "        self.gradb.fill(0)\n",
    "        \n",
    "    def get_parameters(self):\n",
    "        return [self.W, self.b]\n",
    "    \n",
    "    def get_parameters_grad(self):\n",
    "        return [self.gradW, self.gradb]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        s = self.W.shape\n",
    "        q = 'Linear %d -> %d' %(s[1], s[0])\n",
    "        return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHyxnMR_5Con"
   },
   "source": [
    "## 2. SoftMax\n",
    "- input:   **`batch_size x n_feats`**\n",
    "- output: **`batch_size x n_feats`**\n",
    "\n",
    "$\\text{softmax}(x)_i = \\frac{\\exp x_i} {\\sum_j \\exp x_j}$\n",
    "\n",
    "Recall that $\\text{softmax}(x) == \\text{softmax}(x - \\text{const})$. It makes possible to avoid computing exp() from large argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-17T11:24:54.836106Z",
     "start_time": "2022-04-17T11:24:54.824625Z"
    },
    "id": "Ra2UZEqZ5Con"
   },
   "outputs": [],
   "source": [
    "class SoftMax(Module):\n",
    "    def __init__(self):\n",
    "         super(SoftMax, self).__init__()\n",
    "    \n",
    "    def _compute_output(self, input):\n",
    "        # start with normalization for numerical stability\n",
    "        output = np.subtract(input, input.max(axis=1, keepdims=True))\n",
    "        # Your code goes here. ################################################\n",
    "        output = np.divide( np.exp(output), \n",
    "                            np.expand_dims(np.sum(np.exp(output), axis=1), axis=1) )\n",
    "        return output\n",
    "    \n",
    "    def _compute_input_grad(self, input, output_grad):\n",
    "        # Your code goes here. ################################################\n",
    "        s = self._output\n",
    "        s_reshaped = s.reshape(s.shape[0], -1, 1)\n",
    "        eye = np.eye(s.shape[1])\n",
    "        \n",
    "        ds_dx = np.expand_dims(s, axis=-1) * eye - s_reshaped * np.expand_dims(s, 1)\n",
    "        grad_input = np.sum(ds_dx *  np.expand_dims(output_grad, axis=1), axis=2)\n",
    "        \n",
    "        return grad_input\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"SoftMax\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hw7r_fcP5Cop"
   },
   "source": [
    "## 3. LogSoftMax\n",
    "- input:   **`batch_size x n_feats`**\n",
    "- output: **`batch_size x n_feats`**\n",
    "\n",
    "$\\text{logsoftmax}(x)_i = \\log\\text{softmax}(x)_i = x_i - \\log {\\sum_j \\exp x_j}$\n",
    "\n",
    "The main goal of this layer is to be used in computation of log-likelihood loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-17T11:24:55.761554Z",
     "start_time": "2022-04-17T11:24:55.750563Z"
    },
    "id": "-5MaZVah5Coq"
   },
   "outputs": [],
   "source": [
    "class LogSoftMax(Module):\n",
    "    def __init__(self):\n",
    "         super(LogSoftMax, self).__init__()\n",
    "    \n",
    "    def _compute_output(self, input):\n",
    "        # start with normalization for numerical stability\n",
    "        output = np.subtract(input, input.max(axis=1, keepdims=True))\n",
    "\n",
    "        # Your code goes here. ################################################\n",
    "        \n",
    "        # First variant\n",
    "        # output = input - np.log(np.sum(np.exp(input), axis=1)).reshape(-1, 1)\n",
    "        \n",
    "        # Second variant\n",
    "        self.softmax = np.divide( np.exp(output), \n",
    "                                  np.expand_dims(np.sum(np.exp(output), axis=1), axis=1) )\n",
    "        output = np.log(self.softmax)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def _compute_input_grad(self, input, output_grad):\n",
    "        # Your code goes here. ################################################\n",
    "        \n",
    "        batch_size, feat_size = self.softmax.shape\n",
    "        eye = np.eye(feat_size)\n",
    "        \n",
    "        ds_dx = eye - np.repeat(self.softmax, repeats=feat_size, axis=1) \\\n",
    "                        .reshape(batch_size, feat_size, feat_size)\n",
    "\n",
    "        grad_input = np.sum(ds_dx *  np.expand_dims(output_grad, axis=1), axis=2)\n",
    "        \n",
    "        return grad_input\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"LogSoftMax\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybvPD8p05Coq"
   },
   "source": [
    "## 4. Batch normalization\n",
    "One of the most significant recent ideas that impacted NNs a lot is [**Batch normalization**](http://arxiv.org/abs/1502.03167). The idea is simple, yet effective: the features should be whitened ($mean = 0$, $std = 1$) all the way through NN. This improves the convergence for deep models letting it train them for days but not weeks. **You are** to implement the first part of the layer: features normalization. The second part (`ChannelwiseScaling` layer) is implemented below.\n",
    "\n",
    "- input:   **`batch_size x n_feats`**\n",
    "- output: **`batch_size x n_feats`**\n",
    "\n",
    "The layer should work as follows. While training (`self.training == True`) it transforms input as $$y = \\frac{x - \\mu}  {\\sqrt{\\sigma + \\epsilon}}$$\n",
    "where $\\mu$ and $\\sigma$ - mean and variance of feature values in **batch** and $\\epsilon$ is just a small number for numericall stability. Also during training, layer should maintain exponential moving average values for mean and variance: \n",
    "```\n",
    "    self.moving_mean = self.moving_mean * alpha + batch_mean * (1 - alpha)\n",
    "    self.moving_variance = self.moving_variance * alpha + batch_variance * (1 - alpha)\n",
    "```\n",
    "During testing (`self.training == False`) the layer normalizes input using moving_mean and moving_variance. \n",
    "\n",
    "Note that decomposition of batch normalization on normalization itself and channelwise scaling here is just a common **implementation** choice. In general \"batch normalization\" always assumes normalization + scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-17T11:24:56.713864Z",
     "start_time": "2022-04-17T11:24:56.694795Z"
    },
    "id": "XQRFBjo55Cos"
   },
   "outputs": [],
   "source": [
    "class BatchNormalization(Module):\n",
    "    EPS = 1e-3\n",
    "\n",
    "    def __init__(self, alpha=0.):\n",
    "        super(BatchNormalization, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.moving_mean = 0.\n",
    "        self.moving_variance = 1.\n",
    "\n",
    "    def _compute_output(self, input):\n",
    "        # Your code goes here. ################################################\n",
    "        \n",
    "        if self.training:\n",
    "            batch_mean = np.mean(input, axis=0)\n",
    "            batch_var = np.var(input, axis=0)\n",
    "            \n",
    "            output = (input - batch_mean) / np.sqrt(batch_var + self.EPS)\n",
    "            \n",
    "            self.moving_mean = self.moving_mean * self.alpha + batch_mean * (1 - self.alpha)\n",
    "            self.moving_variance = self.moving_variance * self.alpha + batch_var * (1 - self.alpha)\n",
    "        else:\n",
    "            output = (input - self.moving_mean) / np.sqrt(self.moving_variance + self.EPS)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def _compute_input_grad(self, input, output_grad):\n",
    "        # Your code goes here. ################################################\n",
    "        \n",
    "        batch_mean = np.mean(input, axis=0)\n",
    "        batch_var = np.var(input, axis=0)\n",
    "        \n",
    "        dy_dx = (1 / np.sqrt(batch_var + self.EPS)) * output_grad\n",
    "        dy_du = np.sum((- 1 / np.sqrt(batch_var + self.EPS)) * output_grad, axis=0)\n",
    "        dy_dv =  - 1 / 2 *( np.sum( output_grad * (input - batch_mean), axis=0)  / \n",
    "                             np.power((batch_var + self.EPS), 3 / 2) ) \n",
    "\n",
    "        du_dx = 1 / input.shape[0]\n",
    "        \n",
    "        dv_du = np.mean(- 2  * (input - batch_mean), axis=0)\n",
    "        dv_dx = 2 / input.shape[0] * (input - batch_mean)\n",
    "        \n",
    "        grad_input = dy_dx + du_dx * (dy_du + dy_dv * dv_du) + dy_dv * dv_dx \n",
    "        return grad_input\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"BatchNormalization\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "iU8LwKVy5Cot"
   },
   "outputs": [],
   "source": [
    "class ChannelwiseScaling(Module):\n",
    "    \"\"\"\n",
    "       Implements linear transform of input y = \\gamma * x + \\beta\n",
    "       where \\gamma, \\beta - learnable vectors of length x.shape[-1]\n",
    "    \"\"\"\n",
    "    def __init__(self, n_out):\n",
    "        super(ChannelwiseScaling, self).__init__()\n",
    "\n",
    "        stdv = 1./np.sqrt(n_out)\n",
    "        self.gamma = np.random.uniform(-stdv, stdv, size=n_out)\n",
    "        self.beta = np.random.uniform(-stdv, stdv, size=n_out)\n",
    "        \n",
    "        self.gradGamma = np.zeros_like(self.gamma)\n",
    "        self.gradBeta = np.zeros_like(self.beta)\n",
    "\n",
    "    def _compute_output(self, input):\n",
    "        output = input * self.gamma + self.beta\n",
    "        return output\n",
    "        \n",
    "    def _compute_input_grad(self, input, output_grad):\n",
    "        grad_input = output_grad * self.gamma\n",
    "        return grad_input\n",
    "    \n",
    "    def _update_parameters_grad(self, input, output_grad):\n",
    "        self.gradBeta = np.sum(output_grad, axis=0)\n",
    "        self.gradGamma = np.sum(output_grad * input, axis=0)\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.gradGamma.fill(0)\n",
    "        self.gradBeta.fill(0)\n",
    "        \n",
    "    def get_parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "    \n",
    "    def get_parameters_grad(self):\n",
    "        return [self.gradGamma, self.gradBeta]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"ChannelwiseScaling\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lDIWs4955Cou"
   },
   "source": [
    "Practical notes. If BatchNormalization is placed after a linear transformation layer (including dense layer, convolutions, channelwise scaling) that implements function like `y = weight * x + bias`, than bias adding become useless and could be omitted since its effect will be discarded while batch mean subtraction. If BatchNormalization (followed by `ChannelwiseScaling`) is placed before a layer that propagates scale (including ReLU, LeakyReLU) followed by any linear transformation layer than parameter `gamma` in `ChannelwiseScaling` could be freezed since it could be absorbed into the linear transformation layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9Wcfkiu5Cov"
   },
   "source": [
    "## 5. Dropout\n",
    "Implement [**dropout**](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf). The idea and implementation is really simple: just multimply the input by $Bernoulli(p)$ mask. Here $p$ is probability of an element to be zeroed.\n",
    "\n",
    "This has proven to be an effective technique for regularization and preventing the co-adaptation of neurons.\n",
    "\n",
    "While training (`self.training == True`) it should sample a mask on each iteration (for every batch), zero out elements and multiply elements by $1 / (1 - p)$. The latter is needed for keeping mean values of features close to mean values which will be in test mode. When testing this module should implement identity transform i.e. `output = input`.\n",
    "\n",
    "- input:   **`batch_size x n_feats`**\n",
    "- output: **`batch_size x n_feats`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-17T11:37:01.685813Z",
     "start_time": "2022-04-17T11:37:01.675117Z"
    },
    "id": "AxnN67MZ5Cow"
   },
   "outputs": [],
   "source": [
    "class Dropout(Module):\n",
    "    def __init__(self, p=0.5):\n",
    "        super(Dropout, self).__init__()\n",
    "        \n",
    "        self.p = p\n",
    "        self.mask = []\n",
    "        \n",
    "    def _compute_output(self, input):\n",
    "        # Your code goes here. ################################################\n",
    "        \n",
    "        if self.training:\n",
    "            self.mask = np.random.binomial(1, 1 - self.p, size=input.shape)\n",
    "            output = input * self.mask * 1 / (1 - self.p)\n",
    "        else:\n",
    "            output = input\n",
    "            \n",
    "        return output\n",
    "    \n",
    "    def _compute_input_grad(self, input, output_grad):\n",
    "        # Your code goes here. ################################################\n",
    "        \n",
    "        grad_input = output_grad * self.mask * (1 / (1 - self.p))\n",
    "        \n",
    "        return grad_input\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"Dropout\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_GX9gGOg5Cox"
   },
   "source": [
    "# Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8MJPzJVe5Cox"
   },
   "source": [
    "Here's the complete example for the **Rectified Linear Unit** non-linearity (aka **ReLU**): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "Z2NfPlbh5Cox"
   },
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    def __init__(self):\n",
    "         super(ReLU, self).__init__()\n",
    "    \n",
    "    def _compute_output(self, input):\n",
    "        output = np.maximum(input, 0)\n",
    "        return output\n",
    "    \n",
    "    def _compute_input_grad(self, input, output_grad):\n",
    "        grad_input = np.multiply(output_grad , input > 0)\n",
    "        return grad_input\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"ReLU\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KF3rbhco5Cox"
   },
   "source": [
    "## 6. Leaky ReLU\n",
    "Implement [**Leaky Rectified Linear Unit**](http://en.wikipedia.org/wiki%2FRectifier_%28neural_networks%29%23Leaky_ReLUs). Expriment with slope. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-17T12:28:13.915116Z",
     "start_time": "2022-04-17T12:28:13.898055Z"
    },
    "id": "QwO1ftVt5Cox"
   },
   "outputs": [],
   "source": [
    "class LeakyReLU(Module):\n",
    "    def __init__(self, slope = 0.03):\n",
    "        super(LeakyReLU, self).__init__()\n",
    "            \n",
    "        self.slope = slope\n",
    "        \n",
    "    def _compute_output(self, input):\n",
    "        # Your code goes here. ################################################\n",
    "\n",
    "        output = np.where(input < 0, input * self.slope, input)\n",
    "        return output\n",
    "    \n",
    "    def _compute_input_grad(self, input, output_grad):\n",
    "        # Your code goes here. ################################################\n",
    "        \n",
    "        grad_input = np.where(input < 0, self.slope, 1) * output_grad\n",
    "        return grad_input\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"LeakyReLU\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIW1S92a5Coy"
   },
   "source": [
    "## 7. ELU\n",
    "Implement [**Exponential Linear Units**](http://arxiv.org/abs/1511.07289) activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-17T12:32:27.637671Z",
     "start_time": "2022-04-17T12:32:27.627819Z"
    },
    "id": "ReldCTUC5Coy"
   },
   "outputs": [],
   "source": [
    "class ELU(Module):\n",
    "    def __init__(self, alpha = 1.0):\n",
    "        super(ELU, self).__init__()\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def _compute_output(self, input):\n",
    "        # Your code goes here. ################################################\n",
    "        \n",
    "        output = np.where(input < 0, self.alpha * (np.exp(input) - 1), input)\n",
    "        return output\n",
    "    \n",
    "    def _compute_input_grad(self, input, output_grad):\n",
    "        # Your code goes here. ################################################\n",
    "        \n",
    "        grad_input = np.where(input < 0, self.alpha * np.exp(input), 1) * output_grad\n",
    "        return grad_input\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"ELU\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6r6S8FDX5Coy"
   },
   "source": [
    "## 8. SoftPlus\n",
    "Implement [**SoftPlus**](https://en.wikipedia.org/wiki%2FRectifier_%28neural_networks%29) activations. Look, how they look a lot like ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-17T12:34:18.202586Z",
     "start_time": "2022-04-17T12:34:18.192834Z"
    },
    "id": "4TK-xdDD5Coy"
   },
   "outputs": [],
   "source": [
    "class SoftPlus(Module):\n",
    "    def __init__(self):\n",
    "        super(SoftPlus, self).__init__()\n",
    "    \n",
    "    def _compute_output(self, input):\n",
    "        # Your code goes here. ################################################\n",
    "        \n",
    "        output = np.log(1 + np.exp(input))\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def _compute_input_grad(self, input, output_grad):\n",
    "        # Your code goes here. ################################################\n",
    "        \n",
    "        grad_input = np.exp(input) / (1 + np.exp(input)) * output_grad\n",
    "        return grad_input\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"SoftPlus\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9VBQn-K5Coz"
   },
   "source": [
    "# Criterions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "glulPCOs5Coz"
   },
   "source": [
    "Criterions are used to score the models answers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-17T12:39:02.679831Z",
     "start_time": "2022-04-17T12:39:02.665746Z"
    },
    "id": "D5zq9hp_5Coz"
   },
   "outputs": [],
   "source": [
    "class Criterion(object):\n",
    "    def __init__ (self):\n",
    "        self._output = None\n",
    "        self._input_grad = None\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        \"\"\"\n",
    "            Given an input and a target, compute the loss function \n",
    "            associated to the criterion and return the result.\n",
    "            \n",
    "            For consistency this function should not be overrided,\n",
    "            all the code goes in `_compute_output`.\n",
    "        \"\"\"\n",
    "        self._output = self._compute_output(input, target)\n",
    "        return self._output\n",
    "\n",
    "    def backward(self, input, target):\n",
    "        \"\"\"\n",
    "            Given an input and a target, compute the gradients of the loss function\n",
    "            associated to the criterion and return the result. \n",
    "\n",
    "            For consistency this function should not be overrided,\n",
    "            all the code goes in `_compute_input_grad`.\n",
    "        \"\"\"\n",
    "        self._input_grad = self._compute_input_grad(input, target)\n",
    "        return self._input_grad\n",
    "    \n",
    "    def _compute_output(self, input, target):\n",
    "        \"\"\"\n",
    "        Function to override.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _compute_input_grad(self, input, target):\n",
    "        \"\"\"\n",
    "        Returns gradient of input wrt output\n",
    "        \n",
    "        Function to override.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Pretty printing. Should be overrided in every module if you want \n",
    "        to have readable description. \n",
    "        \"\"\"\n",
    "        return \"Criterion\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_h7FqLVl5Coz"
   },
   "source": [
    "The **MSECriterion**, which is basic L2 norm usually used for regression, is implemented here for you.\n",
    "- input:   **`batch_size x n_feats`**\n",
    "- target: **`batch_size x n_feats`**\n",
    "- output: **scalar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-17T12:39:03.344795Z",
     "start_time": "2022-04-17T12:39:03.336890Z"
    },
    "id": "ig_PoZnx5Co0"
   },
   "outputs": [],
   "source": [
    "class MSECriterion(Criterion):\n",
    "    def __init__(self):\n",
    "        super(MSECriterion, self).__init__()\n",
    "        \n",
    "    def _compute_output(self, input, target):   \n",
    "        output = np.sum(np.power(input - target,2)) / input.shape[0]\n",
    "        return output \n",
    " \n",
    "    def _compute_input_grad(self, input, target):\n",
    "        grad  = (input - target) * 2 / input.shape[0]\n",
    "        return grad\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"MSECriterion\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qz0dtHna5Co0"
   },
   "source": [
    "## 9. Negative LogLikelihood criterion (numerically unstable)\n",
    "You task is to implement the **ClassNLLCriterion**. It should implement [multiclass log loss](http://scikit-learn.org/stable/modules/model_evaluation.html#log-loss). Nevertheless there is a sum over `y` (target) in that formula, \n",
    "remember that targets are one-hot encoded. This fact simplifies the computations a lot. Note, that criterions are the only places, where you divide by batch size. Also there is a small hack with adding small number to probabilities to avoid computing log(0).\n",
    "- input:   **`batch_size x n_feats`** - probabilities\n",
    "- target: **`batch_size x n_feats`** - one-hot representation of ground truth\n",
    "- output: **scalar**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-17T12:58:00.813479Z",
     "start_time": "2022-04-17T12:58:00.803155Z"
    },
    "id": "gmW6jiaK5Co0"
   },
   "outputs": [],
   "source": [
    "class ClassNLLCriterionUnstable(Criterion):\n",
    "    EPS = 1e-15\n",
    "    def __init__(self):\n",
    "        a = super(ClassNLLCriterionUnstable, self)\n",
    "        super(ClassNLLCriterionUnstable, self).__init__()\n",
    "        \n",
    "    def _compute_output(self, input, target): \n",
    "        # Use this trick to avoid numerical errors\n",
    "        input_clamp = np.clip(input, self.EPS, 1 - self.EPS)\n",
    "        # Your code goes here. ################################################\n",
    "\n",
    "        output = - np.sum(target * np.log(input_clamp)) / input_clamp.shape[0]\n",
    "        return output\n",
    "\n",
    "    def _compute_input_grad(self, input, target):\n",
    "        # Use this trick to avoid numerical errors\n",
    "        input_clamp = np.clip(input, self.EPS, 1 - self.EPS)\n",
    "        \n",
    "        # Your code goes here. ################################################        \n",
    "        \n",
    "        grad = - target / (input_clamp * input_clamp.shape[0])\n",
    "        return grad\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"ClassNLLCriterionUnstable\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H8sbrlpa5Co0"
   },
   "source": [
    "## 10. Negative LogLikelihood criterion (numerically stable)\n",
    "- input:   **`batch_size x n_feats`** - log probabilities\n",
    "- target: **`batch_size x n_feats`** - one-hot representation of ground truth\n",
    "- output: **scalar**\n",
    "\n",
    "Task is similar to the previous one, but now the criterion input is the output of log-softmax layer. This decomposition allows us to avoid problems with computation of forward and backward of log()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-17T13:02:00.305841Z",
     "start_time": "2022-04-17T13:02:00.301245Z"
    },
    "id": "EESaiWBQ5Co1"
   },
   "outputs": [],
   "source": [
    "class ClassNLLCriterion(Criterion):\n",
    "    def __init__(self):\n",
    "        a = super(ClassNLLCriterion, self)\n",
    "        super(ClassNLLCriterion, self).__init__()\n",
    "        \n",
    "    def _compute_output(self, input, target): \n",
    "        # Your code goes here. ################################################\n",
    "        output = - np.sum(target * input) / input.shape[0]\n",
    "        return output\n",
    "\n",
    "    def _compute_input_grad(self, input, target):\n",
    "        # Your code goes here. ################################################\n",
    "        grad_input = - target / input.shape[0]\n",
    "        return grad_input\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"ClassNLLCriterion\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TBTRnhfd5Co1"
   },
   "source": [
    "# Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(object):\n",
    "    def __init__(self, network):\n",
    "        self._network = network  # contains trainable paramenters and their gradients\n",
    "        self._state = {}  # any information needed to save between optimizer iterations\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Updates network parameters\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yMEgE9hQ5Co1"
   },
   "source": [
    "### SGD optimizer with momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On each step it uses the following formulas for network parameters update:\n",
    "$$v_{t+1} = \\mu * v_t + g_{t+1}$$\n",
    "$$p_{t+1} = p_t - \\alpha * v_{t+1}$$\n",
    "Where $p_t$ - network parameters, $v_t$ - velocity, $\\mu$ - momentum, $\\alpha$ - learning rate, $g_t$ - gradients.\n",
    "\n",
    "Check `torch.optim.SGD` documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UOD3wIyq5Co1"
   },
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    def __init__(self, network, lr, momentum=0.0):\n",
    "        super(SGD, self).__init__(network)\n",
    "        self._learning_rate = lr\n",
    "        self._momentum = momentum\n",
    "        \n",
    "    def step(self):\n",
    "        variables = self._network.get_parameters()\n",
    "        gradients = self._network.get_parameters_grad()\n",
    "        \n",
    "        # 'variables' and 'gradients' have complex structure, accumulated_grads will be stored in a simpler one\n",
    "        self._state.setdefault('accumulated_grads', {})\n",
    "    \n",
    "        var_index = 0 \n",
    "        for current_layer_vars, current_layer_grads in zip(variables, gradients):\n",
    "            for current_var, current_grad in zip(current_layer_vars, current_layer_grads):\n",
    "                old_grad = self._state['accumulated_grads'].setdefault(var_index, np.zeros_like(current_grad))\n",
    "                np.add(self._momentum * old_grad, current_grad, out=old_grad)\n",
    "                current_var -= self._learning_rate * old_grad\n",
    "                var_index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zsN2ACEL5Co1"
   },
   "source": [
    "## 11. [Adam](https://arxiv.org/pdf/1412.6980.pdf) optimizer\n",
    "Formulas for optimizer:\n",
    "\n",
    "Current step learning rate: $$\\text{lr}_t = \\text{learning_rate} * \\frac{\\sqrt{1-\\beta_2^t}} {1-\\beta_1^t}$$\n",
    "First moment of var: $$\\mu_t = \\beta_1 * \\mu_{t-1} + (1 - \\beta_1)*g$$ \n",
    "Second moment of var: $$v_t = \\beta_2 * v_{t-1} + (1 - \\beta_2)*g*g$$\n",
    "New values of var: $$\\text{variable} = \\text{variable} - \\text{lr}_t * \\frac{\\mu_t}{\\sqrt{v_t} + \\epsilon}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam(Optimizer):\n",
    "    def __init__(self, network, lr=0.001, betas=(0.9, 0.999), eps=1e-8):\n",
    "        super(Adam, self).__init__(network)\n",
    "        self._learning_rate = lr\n",
    "        self._beta1 = betas[0]\n",
    "        self._beta2 = betas[1]\n",
    "        self._epsilon = eps\n",
    "        \n",
    "    def step(self):\n",
    "        variables = self._network.get_parameters()\n",
    "        gradients = self._network.get_parameters_grad()\n",
    "        \n",
    "        self._state.setdefault('m', {})  # first moment vars\n",
    "        self._state.setdefault('v', {})  # second moment vars\n",
    "        self._state.setdefault('t', 0)   # timestamp\n",
    "        self._state['t'] += 1\n",
    "        t = self._state['t']\n",
    "    \n",
    "        var_index = 0 \n",
    "        lr_t = self._learning_rate * np.sqrt(1 - self._beta2**t) / (1 - self._beta1**t)\n",
    "        for current_layer_vars, current_layer_grads in zip(variables, gradients): \n",
    "            for current_var, current_grad in zip(current_layer_vars, current_layer_grads):\n",
    "                var_first_moment = self._state['m'].setdefault(var_index, np.zeros_like(current_grad))\n",
    "                var_second_moment = self._state['v'].setdefault(var_index, np.zeros_like(current_grad))\n",
    "\n",
    "                # <YOUR CODE> #######################################\n",
    "                # update `current_var_first_moment`, `var_second_moment` and `current_var` values\n",
    "                #np.add(... , out=var_first_moment)\n",
    "                #np.add(... , out=var_second_moment)\n",
    "                #current_var -= ...\n",
    "\n",
    "                # small checks that you've updated the state; use np.add for rewriting np.arrays values\n",
    "                assert var_first_moment is self._state['m'].get(var_index)\n",
    "                assert var_second_moment is self._state['v'].get(var_index)\n",
    "                var_index += 1"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "1.modules.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
